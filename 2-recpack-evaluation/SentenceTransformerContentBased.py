# pip install --user annoy
# https://github.com/spotify/annoy
import numpy as np
from annoy import AnnoyIndex
from recpack.algorithms.base import Algorithm
from scipy.sparse import lil_matrix, csr_matrix
from sentence_transformers import SentenceTransformer
import logging

# Set more restrictive logging levels for all loggers
logger = logging.getLogger("recpack")
logger.setLevel(logging.ERROR)  # Only show errors, not warnings


class SentenceTransformerContentBased(Algorithm):
    """
    Content-based recommendation algorithm using Sentence Transformers and Annoy.

    Embeds item content using a Sentence Transformer model. User profiles are
    generated by averaging the embeddings of items they interacted with.
    Annoy (Approximate Nearest Neighbors Oh Yeah) is used to find candidate
    items for recommendation based on user profile similarity.

    Args:
        language (str): The Sentence Transformer model to use (e.g., 'all-MiniLM-L6-v2').
                        Defaults to 'all-MiniLM-L6-v2'.
        content (dict): A dictionary mapping item IDs (int) to their textual content (str).
        metric (str): The distance metric for Annoy ('angular', 'euclidean', 'manhattan',
                      'hamming', 'dot'). Defaults to 'dot'.
        embedding_dim (int | None): The dimension of the sentence embeddings. If None,
                                    it's inferred from the model. Defaults to None.
        n_trees (int): Number of trees for the Annoy index. More trees give higher
                       precision but use more memory. Defaults to 10.
        num_neighbors (int): The number of neighbors (recommendations) to retrieve
                             for each user. Defaults to 100.
        verbose (bool): Whether to print detailed progress information. Defaults to False.
    """

    def __init__(self, content: dict, language: str = 'all-MiniLM-L6-v2', metric: str = "dot",
                 embedding_dim: int | None = None, n_trees: int = 10, num_neighbors: int = 100,
                 verbose: bool = False):
        super().__init__()  # Initialize the base Algorithm class

        # Store parameters directly as public attributes for get_params()
        self.content = content
        self.language = language
        self.metric = metric
        # Stores the input parameter (could be None)
        self.embedding_dim = embedding_dim
        self.n_trees = n_trees
        self.num_neighbors = num_neighbors
        self.verbose = verbose

        # Initialize SentenceTransformer
        self.sentencetransformer = SentenceTransformer(self.language)

        # Infer actual embedding dimension if not provided
        if self.embedding_dim is None:
            # Encode a dummy text to get the embedding dimension
            dummy_embedding = self.sentencetransformer.encode("dummy text")
            # Internal actual dimension
            self._embedding_dim = dummy_embedding.shape[0]
        else:
            self._embedding_dim = self.embedding_dim  # Internal actual dimension

        # Initialize Annoy index
        self.annoy_index = AnnoyIndex(self._embedding_dim, self.metric)

        # Internal state attributes
        self._user_offset = None  # Will be calculated in _fit
        self._users_in_annoy = set()  # Keep track of users successfully added
        # Dictionary to store item embeddings for fast access
        self._item_embeddings = {}

        # To track users in the mapping (fixes user_id not in index error)
        self._user_id_map = None

    def _log(self, msg, *args, **kwargs):
        """Print log message only if verbose mode is enabled"""
        if self.verbose:
            print(msg, *args, **kwargs)

    def _fit(self, X: csr_matrix):
        num_U, num_I = X.shape
        # Ensure user IDs are distinct from item IDs using the public attribute
        self._user_offset = num_I + 100
        self._users_in_annoy = set()  # Reset for fitting

        # Initialize user ID mapping for metrics calculation
        self._user_id_map = np.arange(num_U)

        self._log(f"Fitting {self.__class__.__name__}:")
        self._log(f"  Items: {num_I}, Users: {num_U}")
        self._log(f"  Embedding dimension: {self._embedding_dim}")
        self._log(f"  Content dictionary size: {len(self.content)}")

        # 1. Prepare item data for batch encoding
        self._log("  Preparing item data for encoding...")
        item_ids_with_content = []
        content_texts = []
        # Iterate up to num_I to handle items potentially not in self.content
        for item_id in range(num_I):
            text = self.content.get(item_id)
            if text:
                item_ids_with_content.append(item_id)
                content_texts.append(text)
        self._log(
            f"    Found content for {len(item_ids_with_content)}/{num_I} items.")

        # 2. Batch encode item content
        self._log("  Batch encoding item content...")
        if content_texts:
            # Disable progress bar in non-verbose mode
            item_embeddings_array = self.sentencetransformer.encode(
                content_texts, show_progress_bar=self.verbose)
            # Store embeddings in a dictionary for quick lookup
            item_embeddings_dict = {item_id: emb for item_id, emb in zip(
                item_ids_with_content, item_embeddings_array)}
            # Store for later use
            self._item_embeddings = item_embeddings_dict
        else:
            item_embeddings_dict = {}
            self._item_embeddings = {}
            self._log("    No item content found to encode.")

        # 3. Add items with embeddings to Annoy index
        self._log("  Adding items to Annoy index...")
        items_added_to_annoy = 0
        for item_id, embedding in item_embeddings_dict.items():
            # Ensure embedding is numpy array (Annoy expects list or numpy array)
            if not isinstance(embedding, np.ndarray):
                embedding = np.array(embedding)
            self.annoy_index.add_item(item_id, embedding)
            items_added_to_annoy += 1
        self._log(f"    Added {items_added_to_annoy} items to Annoy.")

        # 4. Create user embeddings and add users to Annoy index
        self._log("  Creating user embeddings and adding users to Annoy index...")
        users_added_to_annoy = 0
        for user_id in range(num_U):
            items_interacted_indices = X[user_id].nonzero()[1]

            if len(items_interacted_indices) > 0:
                # Collect embeddings of interacted items that have content
                user_item_embeddings = []
                for item_id in items_interacted_indices:
                    if item_id in item_embeddings_dict:
                        user_item_embeddings.append(
                            item_embeddings_dict[item_id])

                if user_item_embeddings:
                    # Calculate average embedding for the user
                    user_embedding = np.mean(user_item_embeddings, axis=0)
                    # Add the user to the annoy index with an offset
                    annoy_user_id = user_id + self._user_offset
                    self.annoy_index.add_item(annoy_user_id, user_embedding)
                    # Track users successfully added
                    self._users_in_annoy.add(user_id)
                    users_added_to_annoy += 1
        self._log(
            f"    Added {users_added_to_annoy}/{num_U} users to Annoy with valid interaction history.")

        # 5. Build the Annoy index
        self._log("  Building Annoy index...")
        self.annoy_index.build(self.n_trees)

        # Add attributes expected by check_is_fitted
        self.n_features_in_ = num_I
        self.n_users_ = num_U
        self.n_items_ = num_I

        self._log("  Fit complete.")

    # X is often unused in prediction for CB, but kept for interface consistency
    def _predict(self, X: csr_matrix):
        num_U, num_I = X.shape
        if self._user_offset is None:
            raise RuntimeError("The model must be fitted before prediction.")

        result = lil_matrix((num_U, num_I), dtype=np.float32)

        self._log(f"Predicting recommendations for {num_U} users...")
        self._log(f"Items with embeddings: {len(self._item_embeddings)}")
        self._log(f"Users in Annoy index: {len(self._users_in_annoy)}")

        # Determine a reasonable number of candidates to fetch from Annoy
        # Fetching slightly more than num_neighbors helps account for filtering
        num_candidates_to_fetch = min(
            self.num_neighbors * 3, self.annoy_index.get_n_items())

        # Stats for debugging
        users_processed = 0
        users_with_recommendations = 0
        users_without_recommendations = 0

        # Pre-compute a fallback set of popular/diverse items for users with no recommendations
        # Get all items with embeddings as potential fallbacks
        fallback_items = list(self._item_embeddings.keys())
        if not fallback_items:
            # If no item has embeddings, use all possible item IDs as fallback
            fallback_items = list(range(num_I))

        # Shuffle fallback items to ensure diversity
        np.random.shuffle(fallback_items)
        # Take top K as fallback recommendations
        fallback_items = fallback_items[:self.num_neighbors]

        for user_id in range(num_U):
            # Track progress
            users_processed += 1
            if self.verbose and users_processed % 500 == 0:
                self._log(f"  Processed {users_processed}/{num_U} users...")

            # Get current user's interactions to filter them out of recommendations
            user_interactions = set(X[user_id].nonzero()[1])

            try:
                # Two approaches depending on whether user is in the Annoy index
                if user_id in self._users_in_annoy:
                    # Method 1: User exists in annoy index, use get_nns_by_item
                    annoy_user_id = user_id + self._user_offset

                    nn_indices, nn_distances = self.annoy_index.get_nns_by_item(
                        annoy_user_id, num_candidates_to_fetch, search_k=-1, include_distances=True
                    )

                    potential_recs = []

                    for item_idx, dist in zip(nn_indices, nn_distances):
                        # Filter out users (items >= self._user_offset) and already interacted items
                        if item_idx < self._user_offset and item_idx not in user_interactions:
                            # Calculate score based on distance metric
                            score = self._distance_to_similarity(dist)
                            potential_recs.append((item_idx, score))
                else:
                    # Method 2: User not in annoy index, but has interactions in X
                    # Build user embedding on the fly from recent interactions
                    if len(user_interactions) > 0:
                        # Get embeddings for items the user has interacted with
                        user_item_embeddings = []
                        for item_id in user_interactions:
                            if item_id in self._item_embeddings:
                                user_item_embeddings.append(
                                    self._item_embeddings[item_id])

                        # If we found embeddings for at least one item, create user embedding
                        if user_item_embeddings:
                            user_embedding = np.mean(
                                user_item_embeddings, axis=0)

                            # Query annoy directly with the vector
                            nn_indices, nn_distances = self.annoy_index.get_nns_by_vector(
                                user_embedding, num_candidates_to_fetch, search_k=-1, include_distances=True
                            )

                            potential_recs = []

                            for item_idx, dist in zip(nn_indices, nn_distances):
                                # Filter out users and already interacted items
                                if item_idx < self._user_offset and item_idx not in user_interactions:
                                    # Calculate score based on distance metric
                                    score = self._distance_to_similarity(dist)
                                    potential_recs.append((item_idx, score))
                        else:
                            # No embeddings found for user's items
                            potential_recs = []
                    else:
                        # No interactions for this user
                        potential_recs = []

                # Process recommendations if any found
                if potential_recs:
                    # Sort potential recommendations by score (higher is better)
                    potential_recs.sort(key=lambda x: x[1], reverse=True)

                    # Get top N recommendations
                    top_recs = potential_recs[:self.num_neighbors]

                    # Populate the result matrix
                    for item_id, score in top_recs:
                        result[user_id, item_id] = score

                    users_with_recommendations += 1
                else:
                    # FALLBACK: Use pre-computed fallback items for users with no recommendations
                    # Use decreasing scores to maintain a ranking
                    base_score = 0.5  # Start with a medium confidence score
                    score_step = base_score / \
                        len(fallback_items)  # Gradually decrease

                    for i, item_id in enumerate(fallback_items):
                        if item_id not in user_interactions:  # Still filter out items the user has interacted with
                            fallback_score = base_score - \
                                (i * score_step)  # Decreasing scores
                            result[user_id, item_id] = fallback_score

                    users_without_recommendations += 1
                    self._log(
                        f"  Using fallback recommendations for user {user_id}")

            except Exception as e:
                if self.verbose:
                    self._log(f"Error predicting for user {user_id}: {str(e)}")
                    # More detailed error info for debugging
                    import traceback
                    traceback.print_exc()

                # FALLBACK: Even if there's an exception, use fallback recommendations
                base_score = 0.3  # Lower confidence for error cases
                score_step = base_score / len(fallback_items)

                for i, item_id in enumerate(fallback_items):
                    if item_id not in user_interactions:
                        fallback_score = base_score - (i * score_step)
                        result[user_id, item_id] = fallback_score

                users_without_recommendations += 1
                continue

        self._log(
            f"Prediction complete. Made personalized recommendations for {users_with_recommendations}/{num_U} users.")
        self._log(
            f"Used fallback recommendations for {users_without_recommendations}/{num_U} users.")
        return result.tocsr()

    def _distance_to_similarity(self, distance):
        """Convert Annoy distance to similarity score based on the metric."""
        if self.metric == 'angular':
            # For angular (cosine sim), dist = sqrt(2*(1-cos)), so cos = 1 - dist^2 / 2
            return 1.0 - (distance**2) / 2.0
        elif self.metric == 'dot':
            # Similar to angular for normalized vectors
            return 1.0 - (distance**2) / 2.0
        elif self.metric == 'euclidean':
            # Convert Euclidean distance to similarity
            # Short distance = high similarity, using exponential decay
            return np.exp(-distance)
        else:
            # Default conversion for other metrics
            # Normalize to [0,1] range with 0 distance = 1 similarity
            # This is approximate and might need adjustment
            return 1.0 / (1.0 + distance)

    # Add this method to ensure compatibility with recpack metrics
    def _eliminate_empty_users(self, y_true, y_pred):
        """This method ensures users are properly mapped for metrics calculation.
        It overrides the default behavior in recpack.metrics.base.Metric."""
        nonzero_users = list(set(y_true.nonzero()[0]))

        # Store the mapping
        self.user_id_map_ = np.array(nonzero_users)

        return y_true[nonzero_users, :], y_pred[nonzero_users, :]
